{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11\n",
    "\n",
    "\n",
    "## This notebook requires that you have Anaconda installed. Please reach out to me or the TAs if you have questions regarding the installation of Anaconda!\n",
    "\n",
    "Hello class! This week we'll finally be doing some analytics. Specifically, we'll be looking at the housing price data set, which you should download from Google Drive. The file \"data_description.txt\" contains the definition of each colunmn in the data set. We'll introduce the process of reading in data (through Pandas), creating visualization (through matplotlib), and creating a regression model to make predictions (through sklearn). These are essential packages for data scientists using python.\n",
    "\n",
    "This notebook file is meant to be self-contained -- try executing the cells in this notebook one by one and make sure you understand what's going on in each cell and how different components make up the entire data analytics process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data source: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "### Adopted from https://www.dataquest.io/blog/kaggle-getting-started/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing useful packages and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Get a basic understanding of the dataframes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data shape:\", data.shape) #data.shape shows (number of rows, number of columns) stored in the dataframe \"data\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head() # head() show the first few rows in the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the matplotlib package to do visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# this ensures the plots are displayed inside the notebook\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Say we are interested in the SalePrice column in the 'data' dataframe. \n",
    "We can use describe() to check the summary statistics of this column. We can also create a simple histogram to get a sense of the distribution of values in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.SalePrice.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the skewness measure of the column\n",
    "print(\"Skew is:\", data.SalePrice.skew())\n",
    "\n",
    "# create a histogram  in blue\n",
    "plt.hist(data.SalePrice, color='blue')\n",
    "\n",
    "# display the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the plot above, the SalePrice column seems a bit skewed and doesn't have a nice bell shape.\n",
    "We can easily transform the column to make the column more like a bell shape. This can often improve the prediction performance. For example, below we use the log transformation on SalePrice. We can see the skewness measure is a lot smaller, suggesting the column is not as skewed anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.log(data.SalePrice)\n",
    "print (\"Skew is:\", target.skew())\n",
    "plt.hist(target, color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a subset of columns based on data type\n",
    "\n",
    "Pandas DataFrame provides a handy function to help us filter columns we want. For example, suppose we only want to analyze columns containing numerical values (e.g., int, float, etc.). We can call the \"select_dtypes\" function on a DataFrame object and provide the argument \"include=[np.number]\" to tell pandas we only want numerical columns.\n",
    "\n",
    "To get more help on the select_dtypes function, type and execute the following line in a separate cell\n",
    "\n",
    "**help(pd.DataFrame.select_dtypes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate a new DataFrame called numeric_features that only contains numerical columns\n",
    "numeric_features = data.select_dtypes(include=[np.number])\n",
    "\n",
    "# display the data type of each column in this new DataFrame\n",
    "numeric_features.dtypes ## everything is either int or float!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "You can use the corr() function to return the correlation for each pair of columns.\n",
    "Correlation is between -1 and 1. Specifically, 1 means the pair of columns are perfectly positively correlated; -1 means the pair of columns are perfectly negatively correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_features.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save the correlation matrix to a DataFrame called corr\n",
    "corr = numeric_features.corr()\n",
    "print(corr['SalePrice'].sort_values(ascending=False)[:5], '\\n')\n",
    "print(corr['SalePrice'].sort_values(ascending=False)[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Since corr is a DataFrame, we can type corr['SalePrice'] to get the correlation between SalePrice and any other column\n",
    "print(corr['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can choose to print the top 5 columns that have the highest correlation with SalePrice\n",
    "print(corr['SalePrice'].sort_values(ascending=False)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also choose to print the top 5 columns that have the lowest correlation with SalePrice\n",
    "print(corr['SalePrice'].sort_values(ascending=False)[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One way to visualize the relationship between two numerical columns is to create a scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot where the x-axis is GrLivArea, and y-axis is target (log-transformed SalePrice) \n",
    "plt.scatter(x=data['GrLivArea'], y=target, color = 'orange')\n",
    "\n",
    "# Add a label to the y-axis\n",
    "plt.ylabel('Sale Price')\n",
    "\n",
    "# Add a label to the x-axis\n",
    "plt.xlabel('Above grade (ground) living area square feet')\n",
    "\n",
    "# Display the plot: we can see the price increases as the living area (sqft) increases\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same thing to understand the relationship between GarageArea and log(SalePrice)\n",
    "plt.scatter(x=data['GarageArea'], y=target)\n",
    "plt.ylabel('Sale Price')\n",
    "plt.xlabel('Garage Area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw last week, we can easily filter our data based on certain criterion. **Such filtering is often used to get rid of outliers in the data set.** For example, as shown in the previous plot, there are several houses with huge garages but low sale prices that don't seem to be consistent with the general trend. In such cases, it's often useful to remove these outliers so our model can capture the general trend in the data rather than being affected by a small number of outliers.\n",
    "\n",
    "Below, we only keep observations where the GarageArea < 1200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['GarageArea'] < 1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=data['GarageArea'], y=np.log(data.SalePrice))\n",
    "plt.xlim(-200,1600) # This forces the same scale as before\n",
    "plt.ylabel('Sale Price')\n",
    "plt.xlabel('Garage Area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the select_dtypes() function again, this time only returning the columns with non-numercial values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = data.select_dtypes(exclude=[np.number])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, we can analyze these non-numerical columns using the describe() function. As these are now non-numerical columns, the summary statistics returned will be 'count', 'unique' (the number of unique values), 'top' (the most frequent value), 'freq' (the number of times the most frequent value appears)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we investigate a specific column (Street)\n",
    "\n",
    "\n",
    "print (\"Original: \\n\")\n",
    "print (data.Street.value_counts(), \"\\n\") # This shows the frequency of each value in this column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Processing and Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply a function to each observation\n",
    "To apply a certain function to each observation (row) in the DataFrame, we can define the function (e.g., myfunc()) we want to use, and then call **apply(myfunc)**.\n",
    "\n",
    "For example, below we first analyze the *SaleCondition* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.SaleCondition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 6 different values (e.g., Normal, Partial, Abnorml, ...) in this column. Say I only care whether or not a given house's sale condition is Normal (as opposed to forecloruse, family sales, etc.). In other words, I want to create a new column called **Is_Normal**, where Is_Normal = 1 if SaleCondition == 'Normal', and Is_Normal = 0 if SaleCondition is not 'Normal'.\n",
    "\n",
    "What I can do is to create a simple function called check_normal() to return 1 or 0 depending on the value of some variable x. Then I apply this function to the SaleCondition column in the DataFrame by using the following syntax:\n",
    "\n",
    "**data['Is_Normal'] = data.SaleCondition.apply(check_normal)**\n",
    "\n",
    "Notice that in the above line I also made sure to save the returned value (1 or 0) to a new column called 'Is_Normal'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_normal(x):\n",
    "    return 1 if x == 'Normal' else 0\n",
    "data['Is_Normal'] = data.SaleCondition.apply(check_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can then verify that the 'Is_Normal' column has only two values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Is_Normal.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a bar chart to visualize the SalePrice difference between normal houses and abnormal houses. Below we plot the median for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_pivot = data.pivot_table(index='Is_Normal', values='SalePrice', aggfunc=np.median)\n",
    "condition_pivot.plot(kind='bar', color='brown')\n",
    "plt.xlabel('Normal Sale Condition (1) or else (0)')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the \"missingness\" of the data set:\n",
    "One annoying issue we need to address is the presence of missing values (i.e., Null values) in the data. In a real world data set, it is highly likely that not all observations have complete information, and thus we will often see missing values. Below we create a DataFrame to show how many missing values each column contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nulls = pd.DataFrame(data.isnull().sum().sort_values(ascending=False)[:25])\n",
    "nulls.columns = ['Null Count']\n",
    "nulls.index.name = 'Feature'\n",
    "nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolate Missing Values\n",
    "Many models/algorithms require that an observation contain complete information (i.e., no missing value). Any rows with missing values cannot be used by such models and will be dropped. When your data set is not large it's already challenging to make good predictions. In this case you really want to make sure to use all observations you have even though some observations might be missing a variable value here and there.\n",
    "\n",
    "One way to still use such incomplete observations is to **interpolate** missing values. Essentially this is using certain models to *predict* what those missing values should have been, and fill those values in the blanks.\n",
    "\n",
    "Below we show it's easy to interpolate: DataFrame objects have a function called *interpolate()* that does interpolation for you. type *help(pd.DataFrame.interpolate)* to check more interpolate options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(pd.DataFrame.interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the interpolate function to interpolate missing values.\n",
    "# Then save the complete (with missing values filled) data back to the original DataFrame\n",
    "\n",
    "data = data.select_dtypes(include=[np.number]).interpolate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After interpolation, we can verify that there is no longer any missing values in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filling in missing values we are finally ready to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Use linear regression to predict a continuous value\n",
    "\n",
    "One thing we are interested in is whether we can predict a house's sale price using available information in the data set. A simple yet powerful machine learning model we can use is **linear regression**. We will train a linear regression model to let the model make predictions for us.\n",
    "\n",
    "When we say \"train an model\" we really mean that we'll let the model see many different houses. For each house the model will study the house's characteristics and the corresponding sale price. The idea is that after seeing enough houses and their sale prices, the model will be able to *understand* or *learn* the underlying pattern/relationship between SalePrice and other house characteristics on its own. We can then utilize this learned pattern to make predictions for houses that haven't been sold. This is the basic idea behind any machine learning technique. \n",
    "\n",
    "To train a linear regression model to predict the SalePrice, we need to explicitly tell the model that SalePrice is the variable we want to predict (a.k.a., outcome variable, Y variable). We also want to specify the set of variables the model can use to make predictions (a.k.a., predictors, X variables). In our case, the predictors will just be all other variables in the data set except for the house ID, because we don't expect an arbitrary ID number will be predictive of the Sale Price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.SalePrice # predict the SalePrice\n",
    "X = data.drop(['SalePrice', 'Id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small but important digression: The importance of splitting data into a training set and a test set (make sure to do this in your project!)\n",
    "In a typical prediction task, the goal is to train a model that can make accurate predictions of an outcome variable of interest e.g., SalePrice). Machine learning models are very good at finding patterns in a data set. However, the data set that we use for training an model can be quite different from other data sets that we might encounter in the future. Therefore, it is helpful to evaluate how well our model performs on an 'unseen' data set because it can give us a sense of our model performance when used to predict future, unseen data.\n",
    "\n",
    "To achieve this, the most common approach is to **randomly** split our data set into a training set and a test data. We will only use the training set to train our model. And we will then use the trained model to predict the outcome variable in the test set. The model performance on the test set provides an objective estimate of how well our model performs on future unseen data.\n",
    "\n",
    "In other words, when we train the model (e.g., linear regression), we should only use the **training set**; when we evaluate the model performance we should only use the **test set**.\n",
    "\n",
    "The sklearn package allows us to randomly split the data into training and test very easily. Below, we call the train_test_split function, which is part of sklearn package's model_selection utility. The way this function works is that we will provide the predictors (X) and the outcome (Y), a random_state that ensures the random split can be replicated, and the proportion of data that we want to designate as test set (e.g., 0.33 in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                          X, y, random_state=42, test_size=.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the shapes of the resulting training set (67% of data) and test set (33% of data)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Linear Regression\n",
    "\n",
    "With the training and test sets specified, we are ready to train our linear regression model. We will use the **LinearRegression** object defined in the **sklearn** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit a linear regression, simply provide the training set predictors and training set outcome DataFrames as inputs. Then save the trained model as **model** (or any name you like):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take peek at the trained model and see what kind of information is generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we are only interested in **coef_** (coefficients). Let's match each coefficient with its corresponding column. A positive coefficient means the variable is positively correlated with the outcome variable, holding the values of all other columns constant. Similarly, a negative coefficient means the variable is negatively correlated with the outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model.coef_, X.columns, columns=['Coefficient'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the score() function on the model object to assess the performance of our linear regression. The specific measure we generate is called R^2. The larger the R^2, the better the model is. R^2 is between 0 and 1. Notice that we are assessing the model performance on the test data to make sure the estimate is objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"R^2 is: \\n\", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, remember our ultimate goal is to be able to make predictions. This can be achieved easily by calling the predict() function as shown below. Remember, we should predict the data in the test set because those data were not used to train the model and thus more *objective*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess whether the predictions are good, we need to compare the predicted SalePrice with the actual observed SalePrice. Let's put them side by side in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'Actual': y_test, 'Predicted': predictions})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model actually did a pretty good job predicting the SalePrice.\n",
    "\n",
    "Most of the times we want to use an objective metric to measure the prediction performance. One such measures, perhaps the most commonly used one, is called the **root mean squared error (RMSE)**. What it does is that it takes the difference between the actual and prediction for each observation (this is the prediction error for each observation), squares it, then takes the mean of the squared difference across all observations. Then it finally takes the square root of that mean squared difference.\n",
    "\n",
    "Since the RMSE represents some sort of error, **the smaller the better!**\n",
    "\n",
    "This sounds quite complicated, but to compute this metric we can just call a pre-defined mean squared error in sklearn and take the square root of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# compute the RMSE: this represents the average error for a given observation (~40k)\n",
    "print ('RMSE is: \\n', mean_squared_error(y_test, predictions)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a scatterplot to compare actual vs prediction\n",
    "We can also visualize the relationship between the predicted price and the actual price through a scatterplot. Ideally the dots should land on the 45 degree line (y=x):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values = y_test\n",
    "plt.scatter(predictions, actual_values, alpha=.7,\n",
    "            color='b') #alpha helps to show overlapping data\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Actual Price')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, we can see that most observations have quite reasonable predictions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
